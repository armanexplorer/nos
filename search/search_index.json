{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"installation/","text":"Installation !!! warning Before proceeding with `nos` installation, please make sure to meet the requirements described in the [Prerequisites](prerequisites.md) page. You can install nos using Helm 3 (recommended). You can find all the available configuration values in the Chart documentation . helm install oci://ghcr.io/nebuly-ai/helm-charts/nos \\ --version 0.1.2 \\ --namespace nebuly-nos \\ --generate-name \\ --create-namespace Alternatively, you can use Kustomize by cloning the repository and running make deploy . Next steps Getting started with Dynamic MIG Partitioning Getting started with Dynamic MPS Partitioning Getting started with Elastic Resource Quotas","title":"Installation"},{"location":"installation/#installation","text":"!!! warning Before proceeding with `nos` installation, please make sure to meet the requirements described in the [Prerequisites](prerequisites.md) page. You can install nos using Helm 3 (recommended). You can find all the available configuration values in the Chart documentation . helm install oci://ghcr.io/nebuly-ai/helm-charts/nos \\ --version 0.1.2 \\ --namespace nebuly-nos \\ --generate-name \\ --create-namespace Alternatively, you can use Kustomize by cloning the repository and running make deploy .","title":"Installation"},{"location":"installation/#next-steps","text":"Getting started with Dynamic MIG Partitioning Getting started with Dynamic MPS Partitioning Getting started with Elastic Resource Quotas","title":"Next steps"},{"location":"overview/","text":"Overview nos is the open-source module for running AI workloads on Kubernetes in an optimized way, increasing GPU utilization, cutting down infrastructure costs and improving workloads performance. Currently, the available features are: Dynamic GPU partitioning : allow to schedule Pods requesting fractions of GPU. GPU partitioning is performed automatically in real-time based on the Pods pending and running in the cluster, so that Pods can request only the resources that are strictly necessary and GPUs are always fully utilized. Elastic Resource Quota management : increase the number of Pods running on the cluster by allowing namespaces to borrow quotas of reserved resources from other namespaces as long as they are not using them.","title":"Overview"},{"location":"overview/#overview","text":"nos is the open-source module for running AI workloads on Kubernetes in an optimized way, increasing GPU utilization, cutting down infrastructure costs and improving workloads performance. Currently, the available features are: Dynamic GPU partitioning : allow to schedule Pods requesting fractions of GPU. GPU partitioning is performed automatically in real-time based on the Pods pending and running in the cluster, so that Pods can request only the resources that are strictly necessary and GPUs are always fully utilized. Elastic Resource Quota management : increase the number of Pods running on the cluster by allowing namespaces to borrow quotas of reserved resources from other namespaces as long as they are not using them.","title":"Overview"},{"location":"prerequisites/","text":"Prerequisites Kubernetes version 1.23 or newer GPU Support must be enabled Nebuly's device plugin (required only if using MPS partitioning) Cert Manager (optional, but recommended) Enable GPU support Before installing nos , you must enable GPU support in your Kubernetes cluster. There are two ways to do this. One option is to manually install the required components individually,while the other consists in installing only the NVIDIA GPU Operator, which automatically installs all the necessary components for you. See below for more information on these two installation methods. We recommended enabling GPU support using the NVIDIA GPU Operator (option 1). Option 1 - NVIDIA GPU Operator You can install the NVIDIA GPU Operator as follows: helm install --wait --generate-name \\ -n gpu-operator --create-namespace \\ nvidia/gpu-operator --version v22.9.0 \\ --set driver.enabled=true \\ --set migManager.enabled=false \\ --set mig.strategy=mixed \\ --set toolkit.enabled=true Note that the GPU Operator will automatically install a recent version of NVIDIA Drivers and CUDA on all the GPU-enabled nodes of your cluster, so you don't have to manually install them. For further information you can refer to the NVIDIA GPU Operator Documentation . Option 2 - Manual installation !!! warning If you want to enable MPS Dynamic Partitioning, make sure you have a version of CUDA 11.5 or newer installed, as this is the minimum version that supports GPU memory limits in MPS. To enble GPU support in your cluster, you first need to install NVIDIA Drivers and the NVIDIA Container Toolkit on all the nodes of your cluster with a GPU. After installing the NVIDIA Drivers and the Container Toolkit on your nodes, you need to install the following Kubernetes components: NVIDIA GPU Feature Discovery NVIDIA Device Plugin Please note that the configuration parameter migStrategy must be set to mixed (you can do that with --set migStrategy=mixed if you are using Helm). Install Nebuly's device plugin !!! info Nebuly's device plugin is required only if you want to use [dynamic MPS partitioning](#dynamic-gpu-partitioning/getting-started-mps.md). If you don't plan to use MPS partitioning, you can then skip this installation step. You can install Nebuly's device plugin using Helm as follows: helm install oci://ghcr.io/nebuly-ai/helm-charts/nvidia-device-plugin \\ --version 0.13.0 \\ --generate-name \\ -n nebuly-nvidia \\ --create-namespace Nebuly's device plugin runs only on nodes labelled with nos.nebuly.com/gpu-partitioning=mps . If you already have the NVIDIA device plugin installed on your cluster, you need to ensure that only one instance of the device plugin is running on each GPU node (either Nebuly's or NVIDIA's). One way to do that is to add an affinity rule to the NVIDIA device plugin Daemonset so that it doesn't run on any node that has MPS enabled: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nos.nebuly.com/gpu-partitioning operator: NotIn values: - mps For further information you can refer to Nebuly's device plugin documentation .","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"Kubernetes version 1.23 or newer GPU Support must be enabled Nebuly's device plugin (required only if using MPS partitioning) Cert Manager (optional, but recommended)","title":"Prerequisites"},{"location":"prerequisites/#enable-gpu-support","text":"Before installing nos , you must enable GPU support in your Kubernetes cluster. There are two ways to do this. One option is to manually install the required components individually,while the other consists in installing only the NVIDIA GPU Operator, which automatically installs all the necessary components for you. See below for more information on these two installation methods. We recommended enabling GPU support using the NVIDIA GPU Operator (option 1).","title":"Enable GPU support"},{"location":"prerequisites/#option-1-nvidia-gpu-operator","text":"You can install the NVIDIA GPU Operator as follows: helm install --wait --generate-name \\ -n gpu-operator --create-namespace \\ nvidia/gpu-operator --version v22.9.0 \\ --set driver.enabled=true \\ --set migManager.enabled=false \\ --set mig.strategy=mixed \\ --set toolkit.enabled=true Note that the GPU Operator will automatically install a recent version of NVIDIA Drivers and CUDA on all the GPU-enabled nodes of your cluster, so you don't have to manually install them. For further information you can refer to the NVIDIA GPU Operator Documentation .","title":"Option 1 - NVIDIA GPU Operator"},{"location":"prerequisites/#option-2-manual-installation","text":"!!! warning If you want to enable MPS Dynamic Partitioning, make sure you have a version of CUDA 11.5 or newer installed, as this is the minimum version that supports GPU memory limits in MPS. To enble GPU support in your cluster, you first need to install NVIDIA Drivers and the NVIDIA Container Toolkit on all the nodes of your cluster with a GPU. After installing the NVIDIA Drivers and the Container Toolkit on your nodes, you need to install the following Kubernetes components: NVIDIA GPU Feature Discovery NVIDIA Device Plugin Please note that the configuration parameter migStrategy must be set to mixed (you can do that with --set migStrategy=mixed if you are using Helm).","title":"Option 2 - Manual installation"},{"location":"prerequisites/#install-nebulys-device-plugin","text":"!!! info Nebuly's device plugin is required only if you want to use [dynamic MPS partitioning](#dynamic-gpu-partitioning/getting-started-mps.md). If you don't plan to use MPS partitioning, you can then skip this installation step. You can install Nebuly's device plugin using Helm as follows: helm install oci://ghcr.io/nebuly-ai/helm-charts/nvidia-device-plugin \\ --version 0.13.0 \\ --generate-name \\ -n nebuly-nvidia \\ --create-namespace Nebuly's device plugin runs only on nodes labelled with nos.nebuly.com/gpu-partitioning=mps . If you already have the NVIDIA device plugin installed on your cluster, you need to ensure that only one instance of the device plugin is running on each GPU node (either Nebuly's or NVIDIA's). One way to do that is to add an affinity rule to the NVIDIA device plugin Daemonset so that it doesn't run on any node that has MPS enabled: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nos.nebuly.com/gpu-partitioning operator: NotIn values: - mps For further information you can refer to Nebuly's device plugin documentation .","title":"Install Nebuly's device plugin"},{"location":"telemetry/","text":"Sharing feedback to improve nos Open source is a unique resource for sharing knowledge and building great projects collaboratively with the OSS community. To support the development of nos , during the installation of you could share the information strictly necessary to improve the features of this open-source project and facilitate bug detection and fixing. More specifically, you will foster project enhancement by sharing details about the setup and configuration of the environment where you are installing nos and its components. Which data do we collect? We make sure to collect as little data as possible to improve the open-source project: basic information about the Kubernetes cluster Kubernetes version Number of nodes basic information about each node of the cluster Kubelet version Operating system Container runtime Node resources Labels from the NVIDIA GPU Feature Discovery , if present Label node.kubernetes.io/instance-type , if present configuration of nos components values provided during the Helm chart installation Please find below an example of telemetry collection: { \"installationUUID\": \"feb0a960-ed22-4882-96cf-ef0b83deaeb1\", \"nodes\": [ { \"name\": \"node-1\", \"Capacity\": { \"cpu\": \"5\", \"memory\": \"7111996Ki\" }, \"Labels\": { \"nvidia.com/gpu\": \"true\" }, \"NodeInfo\": { \"kernelVersion\": \"5.15.49-linuxkit\", \"osImage\": \"Ubuntu 22.04.1 LTS\", \"containerRuntimeVersion\": \"containerd://1.6.7\", \"kubeletVersion\": \"v1.24.4\", \"architecture\": \"arm64\" } }, { \"name\": \"node-2\", \"Capacity\": { \"cpu\": \"2\", \"memory\": \"7111996Ki\" }, \"Labels\": null, \"NodeInfo\": { \"kernelVersion\": \"5.15.49-linuxkit\", \"osImage\": \"Ubuntu 22.04.1 LTS\", \"containerRuntimeVersion\": \"containerd://1.6.7\", \"kubeletVersion\": \"v1.24.4\", \"architecture\": \"arm64\" } \"chartValues\": { \"allowDefaultNamespace\": false, \"global\": { \"nvidiaGpuResourceMemoryGB\": 32 } }, \"components\": { \"nos-gpu-partitioner\": true, \"nos-scheduler\": true, \"nos-operator\": true } } ] } How to opt-out? You have two possibilities for opting-out: Set the value shareTelemetry to false when installing nos with the Helm Chart bash helm install oci://ghcr.io/nebuly-ai/helm-charts/nos \\ --version 0.1.2 \\ --namespace nebuly-nos \\ --generate-name \\ --create-namespace \\ --set shareTelemetry=false Install nos without using Helm Should I opt out? Being open-source, we have very limited visibility into the use of the tool unless someone actively contacts us or opens an issue on GitHub. We would appreciate it if you would maintain telemetry, as it helps us improve the source code. In fact, it brings increasing value to the project and helps us to better prioritize feature development. We understand that you may still prefer not to share telemetry data and we respect that desire. Please follow the steps above to disable data collection.","title":"Telemetry"},{"location":"telemetry/#sharing-feedback-to-improve-nos","text":"Open source is a unique resource for sharing knowledge and building great projects collaboratively with the OSS community. To support the development of nos , during the installation of you could share the information strictly necessary to improve the features of this open-source project and facilitate bug detection and fixing. More specifically, you will foster project enhancement by sharing details about the setup and configuration of the environment where you are installing nos and its components. Which data do we collect? We make sure to collect as little data as possible to improve the open-source project: basic information about the Kubernetes cluster Kubernetes version Number of nodes basic information about each node of the cluster Kubelet version Operating system Container runtime Node resources Labels from the NVIDIA GPU Feature Discovery , if present Label node.kubernetes.io/instance-type , if present configuration of nos components values provided during the Helm chart installation Please find below an example of telemetry collection: { \"installationUUID\": \"feb0a960-ed22-4882-96cf-ef0b83deaeb1\", \"nodes\": [ { \"name\": \"node-1\", \"Capacity\": { \"cpu\": \"5\", \"memory\": \"7111996Ki\" }, \"Labels\": { \"nvidia.com/gpu\": \"true\" }, \"NodeInfo\": { \"kernelVersion\": \"5.15.49-linuxkit\", \"osImage\": \"Ubuntu 22.04.1 LTS\", \"containerRuntimeVersion\": \"containerd://1.6.7\", \"kubeletVersion\": \"v1.24.4\", \"architecture\": \"arm64\" } }, { \"name\": \"node-2\", \"Capacity\": { \"cpu\": \"2\", \"memory\": \"7111996Ki\" }, \"Labels\": null, \"NodeInfo\": { \"kernelVersion\": \"5.15.49-linuxkit\", \"osImage\": \"Ubuntu 22.04.1 LTS\", \"containerRuntimeVersion\": \"containerd://1.6.7\", \"kubeletVersion\": \"v1.24.4\", \"architecture\": \"arm64\" } \"chartValues\": { \"allowDefaultNamespace\": false, \"global\": { \"nvidiaGpuResourceMemoryGB\": 32 } }, \"components\": { \"nos-gpu-partitioner\": true, \"nos-scheduler\": true, \"nos-operator\": true } } ] }","title":"Sharing feedback to improve nos"},{"location":"telemetry/#how-to-opt-out","text":"You have two possibilities for opting-out: Set the value shareTelemetry to false when installing nos with the Helm Chart bash helm install oci://ghcr.io/nebuly-ai/helm-charts/nos \\ --version 0.1.2 \\ --namespace nebuly-nos \\ --generate-name \\ --create-namespace \\ --set shareTelemetry=false Install nos without using Helm","title":"How to opt-out?"},{"location":"telemetry/#should-i-opt-out","text":"Being open-source, we have very limited visibility into the use of the tool unless someone actively contacts us or opens an issue on GitHub. We would appreciate it if you would maintain telemetry, as it helps us improve the source code. In fact, it brings increasing value to the project and helps us to better prioritize feature development. We understand that you may still prefer not to share telemetry data and we respect that desire. Please follow the steps above to disable data collection.","title":"Should I opt out?"},{"location":"developer/contribution-guidelines/","text":"Contribution guidelines How to submit an issue Did you spot a bug? Did you come up with a cool idea that you think should be implemented? Well, GitHub issues are the best way to let us know! We don't have a strict policy on issue generation: just use a meaningful title and specify the problem or your proposal in the first problem comment. Then, you can use GitHub labels to let us know what kind of proposal you are making, for example bug if you are reporting a new bug or enhancement if you are proposing a library improvement How to contribute to an issue We are always delighted to welcome other people to the contributor section! We are looking forward to welcoming you to the community, but before you rush off and write 1000 lines of code, please take a few minutes to read our tips for contributing to the library. If it's one of your first contributions, check the tag good first issue \ud83c\udfc1 Please fork the library instead of pulling it and creating a new branch. Work on your fork and work on your branch. Do not hesitate to ask questions by commenting on the issue or asking in the community chats. Open a pull request when you think the problem has been solved. In the pull request specify which problems it is solving/closing. For instance, if the pull request solves problem #1, the comment should be Closes #1 . The title of the pull request must be meaningful and self-explanatory. Coding style We use golangci-lint to enforce a consistent coding style. You can run the linter by using the following target: make lint License All the source code files requires a license header. You can add automatically add it to new files by running: make license-fix","title":"Contribution guidelines"},{"location":"developer/contribution-guidelines/#contribution-guidelines","text":"","title":"Contribution guidelines"},{"location":"developer/contribution-guidelines/#how-to-submit-an-issue","text":"Did you spot a bug? Did you come up with a cool idea that you think should be implemented? Well, GitHub issues are the best way to let us know! We don't have a strict policy on issue generation: just use a meaningful title and specify the problem or your proposal in the first problem comment. Then, you can use GitHub labels to let us know what kind of proposal you are making, for example bug if you are reporting a new bug or enhancement if you are proposing a library improvement","title":"How to submit an issue"},{"location":"developer/contribution-guidelines/#how-to-contribute-to-an-issue","text":"We are always delighted to welcome other people to the contributor section! We are looking forward to welcoming you to the community, but before you rush off and write 1000 lines of code, please take a few minutes to read our tips for contributing to the library. If it's one of your first contributions, check the tag good first issue \ud83c\udfc1 Please fork the library instead of pulling it and creating a new branch. Work on your fork and work on your branch. Do not hesitate to ask questions by commenting on the issue or asking in the community chats. Open a pull request when you think the problem has been solved. In the pull request specify which problems it is solving/closing. For instance, if the pull request solves problem #1, the comment should be Closes #1 . The title of the pull request must be meaningful and self-explanatory.","title":"How to contribute to an issue"},{"location":"developer/contribution-guidelines/#coding-style","text":"We use golangci-lint to enforce a consistent coding style. You can run the linter by using the following target: make lint","title":"Coding style"},{"location":"developer/contribution-guidelines/#license","text":"All the source code files requires a license header. You can add automatically add it to new files by running: make license-fix","title":"License"},{"location":"developer/getting-started/","text":"Developer Local development We use Makefile targets for making it easy to setup a local development environment. You can list all the available targets by running make help . Create a local environment You can create a local development environment just by running: make cluster The target uses Kind to create a local Kubernetes cluster that uses Docker containers as nodes. The nos operator uses webhooks that require SSL certificates. You can let cert-manager create and manage them by installing it on the cluster you have created in the previous step: make install-cert-manager Build components You can build the nos components by running the docker-build-<component-name> targets. The targets build the Docker images using the default image name tagged with the version defined in the first line of the Makefile. Optionally, you can override the name and the tag of the Docker image by providing them as argument to the target. Build GPU Partitioner make docker-build-gpu-partitioner make docker-build-gpu-partitioner GPU_PARTITIONER_IMG=custom-image:tag Build Scheduler make docker-build-scheduler make docker-build-scheduler SCHEDULER_IMG=custom-image:tag Build Operator make docker-build-operator make docker-build-operator OPERATOR_IMG=custom-image:tag Build MIG Agent make docker-build-mig-agent make docker-build-mig-agent MIG_AGENT_IMG=custom-image:tag Build GPU Agent make docker-build-gpu-agent make docker-build-gpu-agent GPU_AGENT_IMG=custom-image:tag Load Docker images into the cluster \u26a0\ufe0f If you use the tag latest Kubernetes will always download the image from the registry, ignoring the image you loaded into the cluster. You can load the Docker images you have built in the previous step into the cluster by running: kind load docker-image <image-name>:<image-tag> Install components You can install single nos components by running: make deploy-<component> where <component> is one of the following: - operator - gpu-partitioner - scheduler - mig-agent - gpu-agent The targets above installs the Docker images tagged with the version defined in the first line of the Makefile . You can override the Docker image name and tag by providing it as an argument to the target: make deploy-<component> <COMPONENT>_IMG=<your-image>","title":"Getting started"},{"location":"developer/getting-started/#developer","text":"","title":"Developer"},{"location":"developer/getting-started/#local-development","text":"We use Makefile targets for making it easy to setup a local development environment. You can list all the available targets by running make help .","title":"Local development"},{"location":"developer/getting-started/#create-a-local-environment","text":"You can create a local development environment just by running: make cluster The target uses Kind to create a local Kubernetes cluster that uses Docker containers as nodes. The nos operator uses webhooks that require SSL certificates. You can let cert-manager create and manage them by installing it on the cluster you have created in the previous step: make install-cert-manager","title":"Create a local environment"},{"location":"developer/getting-started/#build-components","text":"You can build the nos components by running the docker-build-<component-name> targets. The targets build the Docker images using the default image name tagged with the version defined in the first line of the Makefile. Optionally, you can override the name and the tag of the Docker image by providing them as argument to the target.","title":"Build components"},{"location":"developer/getting-started/#build-gpu-partitioner","text":"make docker-build-gpu-partitioner make docker-build-gpu-partitioner GPU_PARTITIONER_IMG=custom-image:tag","title":"Build GPU Partitioner"},{"location":"developer/getting-started/#build-scheduler","text":"make docker-build-scheduler make docker-build-scheduler SCHEDULER_IMG=custom-image:tag","title":"Build Scheduler"},{"location":"developer/getting-started/#build-operator","text":"make docker-build-operator make docker-build-operator OPERATOR_IMG=custom-image:tag","title":"Build Operator"},{"location":"developer/getting-started/#build-mig-agent","text":"make docker-build-mig-agent make docker-build-mig-agent MIG_AGENT_IMG=custom-image:tag","title":"Build MIG Agent"},{"location":"developer/getting-started/#build-gpu-agent","text":"make docker-build-gpu-agent make docker-build-gpu-agent GPU_AGENT_IMG=custom-image:tag","title":"Build GPU Agent"},{"location":"developer/getting-started/#load-docker-images-into-the-cluster","text":"\u26a0\ufe0f If you use the tag latest Kubernetes will always download the image from the registry, ignoring the image you loaded into the cluster. You can load the Docker images you have built in the previous step into the cluster by running: kind load docker-image <image-name>:<image-tag>","title":"Load Docker images into the cluster"},{"location":"developer/getting-started/#install-components","text":"You can install single nos components by running: make deploy-<component> where <component> is one of the following: - operator - gpu-partitioner - scheduler - mig-agent - gpu-agent The targets above installs the Docker images tagged with the version defined in the first line of the Makefile . You can override the Docker image name and tag by providing it as an argument to the target: make deploy-<component> <COMPONENT>_IMG=<your-image>","title":"Install components"},{"location":"dynamic-gpu-partitioning/configuration/","text":"Configuration You can customize the GPU Partitioner settings by editing the values file of the nos Helm chart. In this section we focus on some of the values that you would typically want to customize. Pods batch size The GPU partitioner processes pending pods in batches of configurable size. You can set the batch size by editing the following two parameters of the configuration: batchWindowTimeoutSeconds : timeout of the time window used for batching pending Pods. The time window starts when the GPU Partitioner starts processing a batch of pending Pods, and ends when the timeout expires or the batch is completed. batchWindowIdleSeconds : idle time before a batch of pods is considered completed. Once the time window of a batch starts, if idle time elapses and no new pending pods are detected during this time, the batch is considered completed. Increase the value of these two parameters if you want the GPU partitioner to take into account more pending Pods when deciding the GPU partitioning plan, thus making potentially it more effective. Set lower values if you want the partitioning to be performed more frequently (e.g. if you want to react faster to changes in the cluster), and you don't mind if the partitioning is less effective (e.g. the resources requested by some pending pods might not be created). Scheduler configuration The GPU Partitioner uses an internal scheduler to simulate the scheduling of the pending pods to determine whether a candidate GPU partitioning plan would make the pending pods schedulable. The GPU Partitioner reads the scheduler configuration from the ConfigMap defined by the field gpuPartitioner.scheduler.config , and it falls back to the default configuration if the ConfigMap is not found. You can edit this field to provide your custom scheduler configuration. If you installed nos with the scheduler flag enabled, the GPU Partitioner will use its configuration unless you specify a custom ConfigMap. Available MIG geometries The GPU Partitioner determines the most proper partitioning plan to apply by considering the possible MIG geometries allowed each of the GPU models present in the cluster. You can set the MIG geometries supported by each GPU model by editing the gpuPartitioner.knownMigGeometries value of the installation chart . You can edit this file to add new MIG geometries for new GPU models, or to edit the existing ones according to your specific needs. For instance, you can remove some MIG geometries if you don't want to allow them to be used for a certain GPU model. How it works The GPU Partitioner component watches for pending pods that cannot be scheduled due to lack of MIG/MPS resources they request. If it finds such pods, it checks the current partitioning state of the GPUs in the cluster and tries to find a new partitioning state that would allow to schedule them without deleting any of the used resources. It does that by using an internal k8s scheduler, so that before choosing a candidate partitioning, the GPU Partitioner simulates the scheduling to check whether the partitioning would actually allow to schedule the pending Pods. If multiple partitioning configuration can be used to schedule the pending Pods, the one that would result in the highest number of schedulable pods is chosen. Moreover, just in the case of MIG partitioning, each specific GPU model allows to create only certain combinations of MIG profiles, which are called MIG geometries, so the GPU partitioner takes this constraint into account when trying to find a new partitioning. The available MIG geometries of each GPU model are defined in the field gpuPartitioner.knownMigGeometries field of the Helm chart. MIG Partitioning The actual partitioning specified by the GPU Partitioner for MIG GPUs is performed by the MIG Agent, which is a daemonset running on every node labeled with nos.nebuly.com/gpu-partitioning: mig that creates/deletes MIG profiles as requested by the GPU Partitioner. The MIG Agent exposes to the GPU Partitioner the used/free MIG resources of all the GPUs of the node on which it is running through the following node annotations: nos.nebuly.com/status-gpu-<index>-<mig-profile>-free: <quantity> nos.nebuly.com/status-gpu-<index>-<mig-profile>-used: <quantity> The MIG Agent also watches the node's annotations and, every time there desired MIG partitioning specified by the GPU Partitioner does not match the current state, it tries to apply it by creating and deleting the MIG profiles on the target GPUs. The GPU Partitioner specifies the desired MIG geometry of the GPUs of a node through annotations in the following format: nos.nebuly.com/spec-gpu-<index>-<mig-profile>: <quantity> Note that in some cases the MIG Agent might not be able to apply the desired MIG geometry specified by the GPU Partitioner. This can happen for two reasons: the MIG Agent never deletes MIG resources being in use by a Pod some MIG geometries require the MIG profiles to be created in a certain order, and due to reason (1) the MIG Agent might not be able to delete and re-create the existing MIG profiles in the order required by the new MIG geometry. In these cases, the MIG Agent tries to apply the desired partitioning by creating as many required resources as possible, in order to maximize the number of schedulable Pods. This can result in the MIG Agent applying the desired MIG geometry only partially. For further information regarding NVIDIA MIG and its integration with Kubernetes, please refer to the NVIDIA MIG User Guide and to the MIG Support in Kubernetes official documentation provided by NVIDIA. MPS Partitioning The creation and deletion of MPS resources is handled by the k8s-device-plugin, which can expose a single GPU as multiple MPS resources according to its configuration. When allocating a container requesting an MPS resource, the device plugin takes care of injecting theenvironment variables and mounting the volumes required by the container to communicate to the MPS server, making sure that the resource limits defined by the device requested by the container are enforced. For more information about MPS integration with Kubernetes you can refer to the Nebuly k8s-device-plugin documentation.","title":"Configuration"},{"location":"dynamic-gpu-partitioning/configuration/#configuration","text":"You can customize the GPU Partitioner settings by editing the values file of the nos Helm chart. In this section we focus on some of the values that you would typically want to customize.","title":"Configuration"},{"location":"dynamic-gpu-partitioning/configuration/#pods-batch-size","text":"The GPU partitioner processes pending pods in batches of configurable size. You can set the batch size by editing the following two parameters of the configuration: batchWindowTimeoutSeconds : timeout of the time window used for batching pending Pods. The time window starts when the GPU Partitioner starts processing a batch of pending Pods, and ends when the timeout expires or the batch is completed. batchWindowIdleSeconds : idle time before a batch of pods is considered completed. Once the time window of a batch starts, if idle time elapses and no new pending pods are detected during this time, the batch is considered completed. Increase the value of these two parameters if you want the GPU partitioner to take into account more pending Pods when deciding the GPU partitioning plan, thus making potentially it more effective. Set lower values if you want the partitioning to be performed more frequently (e.g. if you want to react faster to changes in the cluster), and you don't mind if the partitioning is less effective (e.g. the resources requested by some pending pods might not be created).","title":"Pods batch size"},{"location":"dynamic-gpu-partitioning/configuration/#scheduler-configuration","text":"The GPU Partitioner uses an internal scheduler to simulate the scheduling of the pending pods to determine whether a candidate GPU partitioning plan would make the pending pods schedulable. The GPU Partitioner reads the scheduler configuration from the ConfigMap defined by the field gpuPartitioner.scheduler.config , and it falls back to the default configuration if the ConfigMap is not found. You can edit this field to provide your custom scheduler configuration. If you installed nos with the scheduler flag enabled, the GPU Partitioner will use its configuration unless you specify a custom ConfigMap.","title":"Scheduler configuration"},{"location":"dynamic-gpu-partitioning/configuration/#available-mig-geometries","text":"The GPU Partitioner determines the most proper partitioning plan to apply by considering the possible MIG geometries allowed each of the GPU models present in the cluster. You can set the MIG geometries supported by each GPU model by editing the gpuPartitioner.knownMigGeometries value of the installation chart . You can edit this file to add new MIG geometries for new GPU models, or to edit the existing ones according to your specific needs. For instance, you can remove some MIG geometries if you don't want to allow them to be used for a certain GPU model.","title":"Available MIG geometries"},{"location":"dynamic-gpu-partitioning/configuration/#how-it-works","text":"The GPU Partitioner component watches for pending pods that cannot be scheduled due to lack of MIG/MPS resources they request. If it finds such pods, it checks the current partitioning state of the GPUs in the cluster and tries to find a new partitioning state that would allow to schedule them without deleting any of the used resources. It does that by using an internal k8s scheduler, so that before choosing a candidate partitioning, the GPU Partitioner simulates the scheduling to check whether the partitioning would actually allow to schedule the pending Pods. If multiple partitioning configuration can be used to schedule the pending Pods, the one that would result in the highest number of schedulable pods is chosen. Moreover, just in the case of MIG partitioning, each specific GPU model allows to create only certain combinations of MIG profiles, which are called MIG geometries, so the GPU partitioner takes this constraint into account when trying to find a new partitioning. The available MIG geometries of each GPU model are defined in the field gpuPartitioner.knownMigGeometries field of the Helm chart.","title":"How it works"},{"location":"dynamic-gpu-partitioning/configuration/#mig-partitioning","text":"The actual partitioning specified by the GPU Partitioner for MIG GPUs is performed by the MIG Agent, which is a daemonset running on every node labeled with nos.nebuly.com/gpu-partitioning: mig that creates/deletes MIG profiles as requested by the GPU Partitioner. The MIG Agent exposes to the GPU Partitioner the used/free MIG resources of all the GPUs of the node on which it is running through the following node annotations: nos.nebuly.com/status-gpu-<index>-<mig-profile>-free: <quantity> nos.nebuly.com/status-gpu-<index>-<mig-profile>-used: <quantity> The MIG Agent also watches the node's annotations and, every time there desired MIG partitioning specified by the GPU Partitioner does not match the current state, it tries to apply it by creating and deleting the MIG profiles on the target GPUs. The GPU Partitioner specifies the desired MIG geometry of the GPUs of a node through annotations in the following format: nos.nebuly.com/spec-gpu-<index>-<mig-profile>: <quantity> Note that in some cases the MIG Agent might not be able to apply the desired MIG geometry specified by the GPU Partitioner. This can happen for two reasons: the MIG Agent never deletes MIG resources being in use by a Pod some MIG geometries require the MIG profiles to be created in a certain order, and due to reason (1) the MIG Agent might not be able to delete and re-create the existing MIG profiles in the order required by the new MIG geometry. In these cases, the MIG Agent tries to apply the desired partitioning by creating as many required resources as possible, in order to maximize the number of schedulable Pods. This can result in the MIG Agent applying the desired MIG geometry only partially. For further information regarding NVIDIA MIG and its integration with Kubernetes, please refer to the NVIDIA MIG User Guide and to the MIG Support in Kubernetes official documentation provided by NVIDIA.","title":"MIG Partitioning"},{"location":"dynamic-gpu-partitioning/configuration/#mps-partitioning","text":"The creation and deletion of MPS resources is handled by the k8s-device-plugin, which can expose a single GPU as multiple MPS resources according to its configuration. When allocating a container requesting an MPS resource, the device plugin takes care of injecting theenvironment variables and mounting the volumes required by the container to communicate to the MPS server, making sure that the resource limits defined by the device requested by the container are enforced. For more information about MPS integration with Kubernetes you can refer to the Nebuly k8s-device-plugin documentation.","title":"MPS Partitioning"},{"location":"dynamic-gpu-partitioning/getting-started-mig/","text":"Getting started with MIG partitioning !!! warning Multi-instance GPU (MIG) mode is supported only by NVIDIA GPUs based on Ampere, Hopper and newer architectures. Prerequisites To enable Dynamic MIG Partitioning on a certain node, the following prerequisites must be met: if a node has multiple GPUs, all the GPUs must be of the same model all the GPUs of the nodes for which you want to enable MIG partitioning must have MIG mode enabled Enable MIG mode By default, MIG is not enabled on GPUs. In order to enable it, SSH into the node and run the following command for each GPU you want to enable MIG, where <index> corresponds to the index of each GPU: sudo nvidia-smi -i <index> -mig 1 Depending on the kind of machine you are using, it may be necessary to reboot the node after enabling MIG mode for one of its GPUs. You can check whether MIG mode has been successfully enabled by running the following command and checking if you get a similar output: $ nvidia-smi -i <index> --query-gpu=pci.bus_id,mig.mode.current --format=csv pci.bus_id, mig.mode.current 00000000:36:00.0, Enabled For more information and troubleshooting you can refer to th NVIDIA documentation . Enable automatic partitioning You can enable automatic MIG partitioning on a node by adding to it the following label: kubectl label nodes <node-name> \"nos.nebuly.com/gpu-partitioning=mig\" The label delegates to nos the management of the MIG resources of all the GPUs of that node, so you don't have to manually configure the MIG geometry of the GPUs anymore: nos will dynamically create and delete the MIG profiles according to the resources requested by the pods submitted to the cluster, within the limits of the possible MIG geometries supported by each GPU model. The available MIG geometries supported by each GPU model are defined in a ConfigMap, which by default contains with the supported geometries of the most popular GPU models. You can override or extend the values of this ConfigMap by editing the field gpuPartitioner.knownMigGeometries of the installation chart . Create pods requesting MIG resources !!! tip There is no need to manually create and manage MIG configurations. You can simply submit your Pods to the cluster and the requested MIG devices are automatically provisioned. You can make your pods request slices of GPU by specifying MIG devices in their containers requests: $ kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: mig-partitioning-example spec: containers: - name: sleepy image: \"busybox:latest\" command: [\"sleep\", \"120\"] resources: limits: nvidia.com/mig-1g.10gb: 1 EOF In the example above, the pod requests a slice of a 10GB of memory, which is the smallest unit available in NVIDIA-A100-80GB-PCIe GPUs. If in your cluster you have different GPU models, the nos might not be able to create the specified MIG resource. You can find the MIG profiles supported by each GPU model in the NVIDIA documentation . !!! note Each container is supposed to request at most one MIG device. If a container needs more resources, then it should ask for a larger, single device as opposed to multiple smaller devices.","title":"Getting started with MIG partitioning"},{"location":"dynamic-gpu-partitioning/getting-started-mig/#getting-started-with-mig-partitioning","text":"!!! warning Multi-instance GPU (MIG) mode is supported only by NVIDIA GPUs based on Ampere, Hopper and newer architectures.","title":"Getting started with MIG partitioning"},{"location":"dynamic-gpu-partitioning/getting-started-mig/#prerequisites","text":"To enable Dynamic MIG Partitioning on a certain node, the following prerequisites must be met: if a node has multiple GPUs, all the GPUs must be of the same model all the GPUs of the nodes for which you want to enable MIG partitioning must have MIG mode enabled","title":"Prerequisites"},{"location":"dynamic-gpu-partitioning/getting-started-mig/#enable-mig-mode","text":"By default, MIG is not enabled on GPUs. In order to enable it, SSH into the node and run the following command for each GPU you want to enable MIG, where <index> corresponds to the index of each GPU: sudo nvidia-smi -i <index> -mig 1 Depending on the kind of machine you are using, it may be necessary to reboot the node after enabling MIG mode for one of its GPUs. You can check whether MIG mode has been successfully enabled by running the following command and checking if you get a similar output: $ nvidia-smi -i <index> --query-gpu=pci.bus_id,mig.mode.current --format=csv pci.bus_id, mig.mode.current 00000000:36:00.0, Enabled For more information and troubleshooting you can refer to th NVIDIA documentation .","title":"Enable MIG mode"},{"location":"dynamic-gpu-partitioning/getting-started-mig/#enable-automatic-partitioning","text":"You can enable automatic MIG partitioning on a node by adding to it the following label: kubectl label nodes <node-name> \"nos.nebuly.com/gpu-partitioning=mig\" The label delegates to nos the management of the MIG resources of all the GPUs of that node, so you don't have to manually configure the MIG geometry of the GPUs anymore: nos will dynamically create and delete the MIG profiles according to the resources requested by the pods submitted to the cluster, within the limits of the possible MIG geometries supported by each GPU model. The available MIG geometries supported by each GPU model are defined in a ConfigMap, which by default contains with the supported geometries of the most popular GPU models. You can override or extend the values of this ConfigMap by editing the field gpuPartitioner.knownMigGeometries of the installation chart .","title":"Enable automatic partitioning"},{"location":"dynamic-gpu-partitioning/getting-started-mig/#create-pods-requesting-mig-resources","text":"!!! tip There is no need to manually create and manage MIG configurations. You can simply submit your Pods to the cluster and the requested MIG devices are automatically provisioned. You can make your pods request slices of GPU by specifying MIG devices in their containers requests: $ kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: mig-partitioning-example spec: containers: - name: sleepy image: \"busybox:latest\" command: [\"sleep\", \"120\"] resources: limits: nvidia.com/mig-1g.10gb: 1 EOF In the example above, the pod requests a slice of a 10GB of memory, which is the smallest unit available in NVIDIA-A100-80GB-PCIe GPUs. If in your cluster you have different GPU models, the nos might not be able to create the specified MIG resource. You can find the MIG profiles supported by each GPU model in the NVIDIA documentation . !!! note Each container is supposed to request at most one MIG device. If a container needs more resources, then it should ask for a larger, single device as opposed to multiple smaller devices.","title":"Create pods requesting MIG resources"},{"location":"dynamic-gpu-partitioning/getting-started-mps/","text":"Getting started with MPS partitioning !!! warning Multi-Process Service (MPS) is supported only by NVIDIA GPUs based on Volta and newer architectures. Prerequisites you need the Nebuly k8s-device-plugin installed on your cluster Enable automatic partitioning You can enable automatic MPS partitioning on a node by adding to it the following label: kubectl label nodes <node-name> \"nos.nebuly.com/gpu-partitioning=mps\" The label delegates to nos the management of the MPS resources of all the GPUs of that node. You just have to create submit your Pods to the cluster and the requested MPS resources are automatically provisioned. Create pods requesting MPS resources You can make your pods request slices of GPU by specifying MPS resources in their containers requests. MPS devices are exposed by our k8s-device-plugin using the following naming convention: nvidia.com/gpu-<size>gb , where <size> corresponds to the GB of memory of the GPU slice. The computing resources are instead equally shared among all its MPS resources. You can specify any size you want, but you should keep in mind that the GPU Partitioner will create an MPS resource on a certain GPU only if its size is smaller or equal than the total amount of memory of that GPU (which is indicated by the node label nvidia.com/gpu.memory applied by the NVIDIA GPU Operator). For instance, you can create a pod requesting a slice of a 10GB of GPU memory as follows: $ kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: mps-partitioning-example spec: hostIPC: true # (2) securityContext: runAsUser: 1000 # (3) containers: - name: sleepy image: \"busybox:latest\" command: [\"sleep\", \"120\"] resources: limits: nvidia.com/gpu-10gb: 1 # (1) EOF Fraction of GPU with 10 GB of memory hostIPC must be set to true Containers must run as the same user as the MPS Server Pods requesting MPS resources must meet two requirements: hostIPC must be set to true in order to allow containers to access the IPC namespace of the host Containers must run as the same user as the user running the MPS server on the host, which is 1000 by default The two requirements above are due to how MPS works. Since it requires the clients and the server to share the same memory space, we need to allow the pods to access the host IPC namespace so that it can communicate with the MPS server running on it. Moreover, the MPS server accepts only connections from clients running as the same user as the server, which is 1000 by default (you can change it by setting the mps.userID value when installing the k8s-device-plugin chart), so the containers of your pods must run with the same user if they request MPS resources. !!! note Containers are supposed to request at most one MPS device. If a container needs more resources, then it should ask for a larger, single device as opposed to multiple smaller devices !!! warning If you run nvidia-smi inside a container, the output still shows the whole memory of the GPU. Nevertheless, processes inside the container are able to allocate only the amount of memory requested by the contaner. You can check the availble GPU memory through the environment variable CUDA_MPS_PINNED_DEVICE_MEM_LIMIT .","title":"Getting started with MPS partitioning"},{"location":"dynamic-gpu-partitioning/getting-started-mps/#getting-started-with-mps-partitioning","text":"!!! warning Multi-Process Service (MPS) is supported only by NVIDIA GPUs based on Volta and newer architectures.","title":"Getting started with MPS partitioning"},{"location":"dynamic-gpu-partitioning/getting-started-mps/#prerequisites","text":"you need the Nebuly k8s-device-plugin installed on your cluster","title":"Prerequisites"},{"location":"dynamic-gpu-partitioning/getting-started-mps/#enable-automatic-partitioning","text":"You can enable automatic MPS partitioning on a node by adding to it the following label: kubectl label nodes <node-name> \"nos.nebuly.com/gpu-partitioning=mps\" The label delegates to nos the management of the MPS resources of all the GPUs of that node. You just have to create submit your Pods to the cluster and the requested MPS resources are automatically provisioned.","title":"Enable automatic partitioning"},{"location":"dynamic-gpu-partitioning/getting-started-mps/#create-pods-requesting-mps-resources","text":"You can make your pods request slices of GPU by specifying MPS resources in their containers requests. MPS devices are exposed by our k8s-device-plugin using the following naming convention: nvidia.com/gpu-<size>gb , where <size> corresponds to the GB of memory of the GPU slice. The computing resources are instead equally shared among all its MPS resources. You can specify any size you want, but you should keep in mind that the GPU Partitioner will create an MPS resource on a certain GPU only if its size is smaller or equal than the total amount of memory of that GPU (which is indicated by the node label nvidia.com/gpu.memory applied by the NVIDIA GPU Operator). For instance, you can create a pod requesting a slice of a 10GB of GPU memory as follows: $ kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: mps-partitioning-example spec: hostIPC: true # (2) securityContext: runAsUser: 1000 # (3) containers: - name: sleepy image: \"busybox:latest\" command: [\"sleep\", \"120\"] resources: limits: nvidia.com/gpu-10gb: 1 # (1) EOF Fraction of GPU with 10 GB of memory hostIPC must be set to true Containers must run as the same user as the MPS Server Pods requesting MPS resources must meet two requirements: hostIPC must be set to true in order to allow containers to access the IPC namespace of the host Containers must run as the same user as the user running the MPS server on the host, which is 1000 by default The two requirements above are due to how MPS works. Since it requires the clients and the server to share the same memory space, we need to allow the pods to access the host IPC namespace so that it can communicate with the MPS server running on it. Moreover, the MPS server accepts only connections from clients running as the same user as the server, which is 1000 by default (you can change it by setting the mps.userID value when installing the k8s-device-plugin chart), so the containers of your pods must run with the same user if they request MPS resources. !!! note Containers are supposed to request at most one MPS device. If a container needs more resources, then it should ask for a larger, single device as opposed to multiple smaller devices !!! warning If you run nvidia-smi inside a container, the output still shows the whole memory of the GPU. Nevertheless, processes inside the container are able to allocate only the amount of memory requested by the contaner. You can check the availble GPU memory through the environment variable CUDA_MPS_PINNED_DEVICE_MEM_LIMIT .","title":"Create pods requesting MPS resources"},{"location":"dynamic-gpu-partitioning/overview/","text":"Overview nos allows you to schedule Pods requesting fractions of GPUs. The GPUs are automatically partitioned into slices that can be requested by individual containers. In this way, GPUs are shared among multiple Pods increasing the overall utilization. The GPUs partitioning is performed automatically in real-time based on the requests of the Pods in your cluster. nos constantly watches the pending Pods and finds the best possible GPU partitioning configuration to schedule the highest number of the ones requesting fractions of GPUs. You can think of nos as a Cluster Autoscaler for GPUs: instead of adjusting the number of nodes and GPUs, it dynamically partitions them to maximize their utilization, leading to spare GPU capacity. Then, you can schedule more Pods or reduce the number of GPU nodes needed, reducing infrastructure costs. The GPU partitioning is performed either using Multi-instance GPU (MIG) or Multi-Process Service (MPS) , depending on the partitioning mode you choose for each node.","title":"Overview"},{"location":"dynamic-gpu-partitioning/overview/#overview","text":"nos allows you to schedule Pods requesting fractions of GPUs. The GPUs are automatically partitioned into slices that can be requested by individual containers. In this way, GPUs are shared among multiple Pods increasing the overall utilization. The GPUs partitioning is performed automatically in real-time based on the requests of the Pods in your cluster. nos constantly watches the pending Pods and finds the best possible GPU partitioning configuration to schedule the highest number of the ones requesting fractions of GPUs. You can think of nos as a Cluster Autoscaler for GPUs: instead of adjusting the number of nodes and GPUs, it dynamically partitions them to maximize their utilization, leading to spare GPU capacity. Then, you can schedule more Pods or reduce the number of GPU nodes needed, reducing infrastructure costs. The GPU partitioning is performed either using Multi-instance GPU (MIG) or Multi-Process Service (MPS) , depending on the partitioning mode you choose for each node.","title":"Overview"},{"location":"dynamic-gpu-partitioning/partitioning-modes-comparison/","text":"Partitioning modes comparison The following tables summarizes the difference between the different partitioning modes supported by NVIDIA GPUs. Note that they are not mutually exclusive: nos allows you to choose a different partitioning mode for each node in your cluster according to your needs and available hardware. Partitioning mode Supported by nos Workload isolation level Pros Cons Multi-instance GPU (MIG) \u2705 Best Processes are executed in parallel Full isolation (dedicated memory and compute resources) Supported by fewer GPU models (only Ampere or more recent architectures) Coarse-grained control over memory and compute resources Multi-process server (MPS) \u2705 Medium Processes are executed parallel Fine-grained control over memory and compute resources allocation No error isolation and memory protection Time-slicing \u274c None Processes are executed concurrently Supported by older GPU architectures (Pascal or newer) No resource limits No memory isolation Lower performance due to context-switching overhead Multi-instance GPU (MIG) Multi-instance GPU (MIG) is a technology available on NVIDIA Ampere or more recent architectures that allows to securely partition a GPU into separate GPU instances for CUDA applications, each fully isolated with its own high-bandwidth memory, cache, and compute cores. The isolated GPU slices are called MIG devices, and they are named adopting a format that indicates the compute and memory resources of the device. For example, 2g.20gb corresponds to a GPU slice with 20 GB of memory. MIG does not allow to create GPU slices of custom sizes and quantity, as each GPU model only supports a specific set of MIG profiles . This reduces the degree of granularity with which you can partition the GPUs. Additionally, the MIG devices must be created respecting certain placement rules, which further limits flexibility of use. MIG is the GPU sharing approach that offers the highest level of isolation among processes. However, it lacks in flexibility and it is compatible only with few GPU architectures (Ampere and Hopper). You can find out more on how MIG technology works in the official NVIDIA MIG User Guide . Multi-Process Service (MPS) Multi-Process Service (MPS) is a client-server implementation of the CUDA Application Programming Interface (API) for running multiple processes concurrently on the same GPU: the server manages GPU access providing concurrency between clients clients connect to the server through the client runtime, which is built into the CUDA Driver library and may be used transparently by any CUDA application. The main advantage of MPS is that it provides a fine-grained control over the GPU assigned to each client, allowing to specify arbitrary limits on both the amount of allocatable memory and the available compute. The Nebuly k8s-device-plugin takes advantage of this feature for exposing to Kubernetes GPU resources with an arbitrary amount of allocatable memory defined by the user. Compared to time-slicing, MPS eliminates the overhead of context-switching by running processes in parallel through spatial sharing, and therefore leads to better compute performance. Moreover, MPS provides each client with its own GPU memory address space. This allows to enforce memory limits on the processes overcoming the limitations of time-slicing sharing. It is however important to point out that processes sharing a GPU through MPS are not fully isolated from each other. Indeed, even though MPS allows to limit clients' compute and memory resources, it does not provide error isolation and memory protection. This means that a client process can crash and cause the entire GPU to reset, impacting all other processes running on the GPU. However, this issue can often be addressed by properly handling CUDA errors and SIGTERM signals. Time-slicing Time-slicing consists of oversubscribing a GPU leveraging its time-slicing scheduler, which executes multiple CUDA processes concurrently through temporal sharing . This means that the GPU shares its compute resources among the different processes in a fair-sharing manner by switching between processes at regular intervals of time. This generates a computing time overhead related to the continuous context switching, which translates into jitter and higher latency. Time-slicing is supported by basically every GPU architecture and is the simplest solution for sharing a GPU in a Kubernetes cluster. However, constant switching among processes creates a computation time overhead. Also, time-slicing does not provide any level of memory isolation among the processes sharing a GPU, nor any memory allocation limits, which can lead to frequent Out-Of-Memory (OOM) errors. !!! info Given the drawbacks above the availability of more robust technologies such as MIG and MPS, at the moment we decided to not support time-slicing GPU sharing in nos .","title":"Partitioning modes comparison"},{"location":"dynamic-gpu-partitioning/partitioning-modes-comparison/#partitioning-modes-comparison","text":"The following tables summarizes the difference between the different partitioning modes supported by NVIDIA GPUs. Note that they are not mutually exclusive: nos allows you to choose a different partitioning mode for each node in your cluster according to your needs and available hardware. Partitioning mode Supported by nos Workload isolation level Pros Cons Multi-instance GPU (MIG) \u2705 Best Processes are executed in parallel Full isolation (dedicated memory and compute resources) Supported by fewer GPU models (only Ampere or more recent architectures) Coarse-grained control over memory and compute resources Multi-process server (MPS) \u2705 Medium Processes are executed parallel Fine-grained control over memory and compute resources allocation No error isolation and memory protection Time-slicing \u274c None Processes are executed concurrently Supported by older GPU architectures (Pascal or newer) No resource limits No memory isolation Lower performance due to context-switching overhead","title":"Partitioning modes comparison"},{"location":"dynamic-gpu-partitioning/partitioning-modes-comparison/#multi-instance-gpu-mig","text":"Multi-instance GPU (MIG) is a technology available on NVIDIA Ampere or more recent architectures that allows to securely partition a GPU into separate GPU instances for CUDA applications, each fully isolated with its own high-bandwidth memory, cache, and compute cores. The isolated GPU slices are called MIG devices, and they are named adopting a format that indicates the compute and memory resources of the device. For example, 2g.20gb corresponds to a GPU slice with 20 GB of memory. MIG does not allow to create GPU slices of custom sizes and quantity, as each GPU model only supports a specific set of MIG profiles . This reduces the degree of granularity with which you can partition the GPUs. Additionally, the MIG devices must be created respecting certain placement rules, which further limits flexibility of use. MIG is the GPU sharing approach that offers the highest level of isolation among processes. However, it lacks in flexibility and it is compatible only with few GPU architectures (Ampere and Hopper). You can find out more on how MIG technology works in the official NVIDIA MIG User Guide .","title":"Multi-instance GPU (MIG)"},{"location":"dynamic-gpu-partitioning/partitioning-modes-comparison/#multi-process-service-mps","text":"Multi-Process Service (MPS) is a client-server implementation of the CUDA Application Programming Interface (API) for running multiple processes concurrently on the same GPU: the server manages GPU access providing concurrency between clients clients connect to the server through the client runtime, which is built into the CUDA Driver library and may be used transparently by any CUDA application. The main advantage of MPS is that it provides a fine-grained control over the GPU assigned to each client, allowing to specify arbitrary limits on both the amount of allocatable memory and the available compute. The Nebuly k8s-device-plugin takes advantage of this feature for exposing to Kubernetes GPU resources with an arbitrary amount of allocatable memory defined by the user. Compared to time-slicing, MPS eliminates the overhead of context-switching by running processes in parallel through spatial sharing, and therefore leads to better compute performance. Moreover, MPS provides each client with its own GPU memory address space. This allows to enforce memory limits on the processes overcoming the limitations of time-slicing sharing. It is however important to point out that processes sharing a GPU through MPS are not fully isolated from each other. Indeed, even though MPS allows to limit clients' compute and memory resources, it does not provide error isolation and memory protection. This means that a client process can crash and cause the entire GPU to reset, impacting all other processes running on the GPU. However, this issue can often be addressed by properly handling CUDA errors and SIGTERM signals.","title":"Multi-Process Service (MPS)"},{"location":"dynamic-gpu-partitioning/partitioning-modes-comparison/#time-slicing","text":"Time-slicing consists of oversubscribing a GPU leveraging its time-slicing scheduler, which executes multiple CUDA processes concurrently through temporal sharing . This means that the GPU shares its compute resources among the different processes in a fair-sharing manner by switching between processes at regular intervals of time. This generates a computing time overhead related to the continuous context switching, which translates into jitter and higher latency. Time-slicing is supported by basically every GPU architecture and is the simplest solution for sharing a GPU in a Kubernetes cluster. However, constant switching among processes creates a computation time overhead. Also, time-slicing does not provide any level of memory isolation among the processes sharing a GPU, nor any memory allocation limits, which can lead to frequent Out-Of-Memory (OOM) errors. !!! info Given the drawbacks above the availability of more robust technologies such as MIG and MPS, at the moment we decided to not support time-slicing GPU sharing in nos .","title":"Time-slicing"},{"location":"dynamic-gpu-partitioning/troubleshooting/","text":"Troubleshooting If you run into issues with Automatic GPU Partitioning, you can troubleshoot by checking the logs of the GPU Partitioner and MIG Agent pods. You can do that by running the following commands: Check GPU Partitioner logs: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-gpu-partitioner -f Check MIG Agent logs: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-mig-agent -f Check Nebuly's device-plugin logs: kubectl logs -n nebuly-nvidia -l app.kubernetes.io/name=nebuly-nvidia-device-plugin -f","title":"Troubleshooting"},{"location":"dynamic-gpu-partitioning/troubleshooting/#troubleshooting","text":"If you run into issues with Automatic GPU Partitioning, you can troubleshoot by checking the logs of the GPU Partitioner and MIG Agent pods. You can do that by running the following commands: Check GPU Partitioner logs: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-gpu-partitioner -f Check MIG Agent logs: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-mig-agent -f Check Nebuly's device-plugin logs: kubectl logs -n nebuly-nvidia -l app.kubernetes.io/name=nebuly-nvidia-device-plugin -f","title":"Troubleshooting"},{"location":"elastic-resource-quota/configuration/","text":"Configuration Scheduler installation options You can add scheduling support for Elastic Resource Quota to your cluster by choosing one of the following options. In both cases, you also need to install the nos operator to manage the CRDs. Option 1 - Use nos scheduler (recommended) This is the recommended option. You can deploy the nos scheduler to your cluster either as the default scheduler or as a second scheduler that runs alongside the default one. In the latter case, you can use the schedulerName field of the Pod spec to specify which scheduler should be used. If you installed nos through the Helm chart, the scheduler is deployed automatically unless you set the value scheduler.enabled=false . Option 2 - Use your k8s scheduler Since nos Elastic Quota support is implemented as a scheduler plugin, you can compile it into your k8s scheduler and then enable it through the kube-scheduler configuration as follows: apiVersion: kubescheduler.config.k8s.io/v1beta2 kind: KubeSchedulerConfiguration leaderElection: leaderElect: false profiles: - schedulerName: default-scheduler plugins: preFilter: enabled: - name: CapacityScheduling postFilter: enabled: - name: CapacityScheduling disabled: - name: \"*\" reserve: enabled: - name: CapacityScheduling pluginConfig: - name: CapacityScheduling args: # Defines how much GB of memory does a nvidia.com/gpu has. nvidiaGpuResourceMemoryGB: 32 In order to compile the plugin with your scheduler, you just need to add the following line to the main.go file of your scheduler: package main import ( \"github.com/nebuly-ai/nos/pkg/scheduler/plugins/capacityscheduling\" \"k8s.io/kubernetes/cmd/kube-scheduler/app\" // Import plugin config \"github.com/nebuly-ai/nos/pkg/api/scheduler\" \"github.com/nebuly-ai/nos/pkg/api/scheduler/v1beta3\" // Ensure nos.nebuly.com/v1alpha1 package is initialized _ \"github.com/nebuly-ai/nos/pkg/api/nos.nebuly.com/v1alpha1\" ) func main() { // - rest of your code here - // Add plugin config to scheme utilruntime.Must(scheduler.AddToScheme(scheme)) utilruntime.Must(v1beta3.AddToScheme(scheme)) // Add plugin to scheduler command command := app.NewSchedulerCommand( // - your other plugins here - app.WithPlugin(capacityscheduling.Name, capacityscheduling.New), ) // - rest of your code here - } If you choose this installation option, you don't need to deploy nos scheduler, so you can disable it by setting --set scheduler.enabled=false when installing the nos chart.","title":"Configuration"},{"location":"elastic-resource-quota/configuration/#configuration","text":"","title":"Configuration"},{"location":"elastic-resource-quota/configuration/#scheduler-installation-options","text":"You can add scheduling support for Elastic Resource Quota to your cluster by choosing one of the following options. In both cases, you also need to install the nos operator to manage the CRDs.","title":"Scheduler installation options"},{"location":"elastic-resource-quota/configuration/#option-1-use-nos-scheduler-recommended","text":"This is the recommended option. You can deploy the nos scheduler to your cluster either as the default scheduler or as a second scheduler that runs alongside the default one. In the latter case, you can use the schedulerName field of the Pod spec to specify which scheduler should be used. If you installed nos through the Helm chart, the scheduler is deployed automatically unless you set the value scheduler.enabled=false .","title":"Option 1 - Use nos scheduler (recommended)"},{"location":"elastic-resource-quota/configuration/#option-2-use-your-k8s-scheduler","text":"Since nos Elastic Quota support is implemented as a scheduler plugin, you can compile it into your k8s scheduler and then enable it through the kube-scheduler configuration as follows: apiVersion: kubescheduler.config.k8s.io/v1beta2 kind: KubeSchedulerConfiguration leaderElection: leaderElect: false profiles: - schedulerName: default-scheduler plugins: preFilter: enabled: - name: CapacityScheduling postFilter: enabled: - name: CapacityScheduling disabled: - name: \"*\" reserve: enabled: - name: CapacityScheduling pluginConfig: - name: CapacityScheduling args: # Defines how much GB of memory does a nvidia.com/gpu has. nvidiaGpuResourceMemoryGB: 32 In order to compile the plugin with your scheduler, you just need to add the following line to the main.go file of your scheduler: package main import ( \"github.com/nebuly-ai/nos/pkg/scheduler/plugins/capacityscheduling\" \"k8s.io/kubernetes/cmd/kube-scheduler/app\" // Import plugin config \"github.com/nebuly-ai/nos/pkg/api/scheduler\" \"github.com/nebuly-ai/nos/pkg/api/scheduler/v1beta3\" // Ensure nos.nebuly.com/v1alpha1 package is initialized _ \"github.com/nebuly-ai/nos/pkg/api/nos.nebuly.com/v1alpha1\" ) func main() { // - rest of your code here - // Add plugin config to scheme utilruntime.Must(scheduler.AddToScheme(scheme)) utilruntime.Must(v1beta3.AddToScheme(scheme)) // Add plugin to scheduler command command := app.NewSchedulerCommand( // - your other plugins here - app.WithPlugin(capacityscheduling.Name, capacityscheduling.New), ) // - rest of your code here - } If you choose this installation option, you don't need to deploy nos scheduler, so you can disable it by setting --set scheduler.enabled=false when installing the nos chart.","title":"Option 2 - Use your k8s scheduler"},{"location":"elastic-resource-quota/getting-started/","text":"Getting started Create elastic quotas $ kubectl apply -f -- <<EOF apiVersion: nos.nebuly.com/v1alpha1 kind: ElasticQuota metadata: name: quota-a namespace: team-a spec: min: cpu: 2 nos.nebuly.com/gpu-memory: 16 max: cpu: 10 EOF The example above creates a quota for the namespace team-a , guaranteeing it 2 CPUs and 16 GB of GPU memory, and limiting the maximum number of CPUs it can use to 10. Note that: the max field is optional. If it is not specified, then the Elastic Quota does not enforce any upper limits on the amount resources that can be created in the namespace you can specify any valid Kubernetes resource you want in max and min fields Create Pods subject to Elastic Resource Quota Unless you deployed the nos scheduler as the default scheduler for your cluster, you need to instruct Kubernetes to use it for scheduling the Pods you want to be subject to Elastic Resource Quotas. You can do that by setting the value of the schedulerName field of your Pods specification to scheduler (or to any name you chose when installing nos ), as shown in the example below. apiVersion: v1 kind: Pod metadata: name: my-pod spec: schedulerName: nos-scheduler containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 How to define resource quotas You can define resource limits on namespaces using two custom resources: ElasticQuota and CompositeElasticQuota . They both work in the same way, the only difference is that the latter defines limits on multiple namespaces instead of on a single one. Limits are specified through two fields: min : the minimum resources that are guaranteed to the namespace. nos will make sure that, at any time, the namespace subject to the quota will always have access to at least these resources. max : optional field that limits the total amount of resources that can be requested by a namespace. If not max is not specified, then nos does not enforce any upper limits on the resources that can be requested by the namespace. You can find sample definitions of these resources under the samples directory. Note that ElasticQuota and CompositeElasticQuota are treated by nos in the same way: a namespace subject to an ElasticQuota can borrow resources from namespaces subject to either other elastic quotas or composite elastic quotas and, vice-versa, namespaces subject to a CompositeElasticQuota can borrow resources from namespaces subject to either elastic quotas or composite elastic quotas. Constraints The following constraints are enforced over elastic quota resources: you can create at most one ElasticQuota per namespace a namespace can be subject either to one ElasticQuota or one CompositeElasticQuota , but not both at the same time if a quota resource specifies both max and min fields, then the value of the resources specified in max must be greater or equal than the ones specified in min How used resources are computed When a namespace is subject to an ElasticQuota (or to a CompositeElasticQuota), nos computes the number of quotas consumed by that namespace by aggregating the resources requested by its pods, considering only the ones whose phase is Running . In this way, nos avoid lower resource utilization due to scheduled pods that failed to start. Every time the amount of resources consumed by a namespace changes (e.g a Pod changes its phase to or from Running ), the status of the respective quota object gets updated with the new amount of used resources. You can check how many resources have been consumed by each namespace by looking at the field used of the ElasticQuota and CompositeElasticQuota objects status.","title":"Getting started"},{"location":"elastic-resource-quota/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"elastic-resource-quota/getting-started/#create-elastic-quotas","text":"$ kubectl apply -f -- <<EOF apiVersion: nos.nebuly.com/v1alpha1 kind: ElasticQuota metadata: name: quota-a namespace: team-a spec: min: cpu: 2 nos.nebuly.com/gpu-memory: 16 max: cpu: 10 EOF The example above creates a quota for the namespace team-a , guaranteeing it 2 CPUs and 16 GB of GPU memory, and limiting the maximum number of CPUs it can use to 10. Note that: the max field is optional. If it is not specified, then the Elastic Quota does not enforce any upper limits on the amount resources that can be created in the namespace you can specify any valid Kubernetes resource you want in max and min fields","title":"Create elastic quotas"},{"location":"elastic-resource-quota/getting-started/#create-pods-subject-to-elastic-resource-quota","text":"Unless you deployed the nos scheduler as the default scheduler for your cluster, you need to instruct Kubernetes to use it for scheduling the Pods you want to be subject to Elastic Resource Quotas. You can do that by setting the value of the schedulerName field of your Pods specification to scheduler (or to any name you chose when installing nos ), as shown in the example below. apiVersion: v1 kind: Pod metadata: name: my-pod spec: schedulerName: nos-scheduler containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80","title":"Create Pods subject to Elastic Resource Quota"},{"location":"elastic-resource-quota/getting-started/#how-to-define-resource-quotas","text":"You can define resource limits on namespaces using two custom resources: ElasticQuota and CompositeElasticQuota . They both work in the same way, the only difference is that the latter defines limits on multiple namespaces instead of on a single one. Limits are specified through two fields: min : the minimum resources that are guaranteed to the namespace. nos will make sure that, at any time, the namespace subject to the quota will always have access to at least these resources. max : optional field that limits the total amount of resources that can be requested by a namespace. If not max is not specified, then nos does not enforce any upper limits on the resources that can be requested by the namespace. You can find sample definitions of these resources under the samples directory. Note that ElasticQuota and CompositeElasticQuota are treated by nos in the same way: a namespace subject to an ElasticQuota can borrow resources from namespaces subject to either other elastic quotas or composite elastic quotas and, vice-versa, namespaces subject to a CompositeElasticQuota can borrow resources from namespaces subject to either elastic quotas or composite elastic quotas.","title":"How to define resource quotas"},{"location":"elastic-resource-quota/getting-started/#constraints","text":"The following constraints are enforced over elastic quota resources: you can create at most one ElasticQuota per namespace a namespace can be subject either to one ElasticQuota or one CompositeElasticQuota , but not both at the same time if a quota resource specifies both max and min fields, then the value of the resources specified in max must be greater or equal than the ones specified in min","title":"Constraints"},{"location":"elastic-resource-quota/getting-started/#how-used-resources-are-computed","text":"When a namespace is subject to an ElasticQuota (or to a CompositeElasticQuota), nos computes the number of quotas consumed by that namespace by aggregating the resources requested by its pods, considering only the ones whose phase is Running . In this way, nos avoid lower resource utilization due to scheduled pods that failed to start. Every time the amount of resources consumed by a namespace changes (e.g a Pod changes its phase to or from Running ), the status of the respective quota object gets updated with the new amount of used resources. You can check how many resources have been consumed by each namespace by looking at the field used of the ElasticQuota and CompositeElasticQuota objects status.","title":"How used resources are computed"},{"location":"elastic-resource-quota/key-concepts/","text":"Key concepts Over-quotas If a namespace subject to an ElasticQuota (or, equivalently, to a CompositeElasticQuota ) is using all the resources guaranteed by the min field of its quota, it can still host new pods by \"borrowing\" quotas from other namespaces which has available resources (e.g. from namespaces subject to other quotas where min resources are not being completely used). !!! info Pods that are scheduled \"borrowing\" unused quotas from other namespaces are called **over-quota pods**. Over-quota pods can be preempted at any time to free up resources if any of the namespaces lending the quotas claims back its resources. You can check whether a Pod is in over-quota by checking the value of the label nos.nebuly.com/capacity , which is automatically created and updated by the nos operator for every Pod created in a namespace subject to an ElasticQuota or to a CompositeElasticQuota. The two possible values for this label are in-quota and over-quota . You can use this label to easily find out at any time which are the over-quota pods subject to preemption risk: kubectl get pods --all-namespaces -l nos.nebuly.com/capacity=\"over-quota\" How over-quota pods are labelled All the pods created within a namespace subject to a quota are labelled as in-quota as long as the used resources of the quota do not exceed its min resources. When this happens and news pods are created in that namespace, they are labelled as over-quota when they reach the running phase. nos re-evaluates the over-quota status of each Pod of a namespace every time a new Pod in that namespace changes its phase to/from \"Running\". With the default configuration, nos sorts the pods by creation date and, if the creation timestamp is the same, by requested resources, placing first the pods with older creation timestamp and with fewer requested resources. After the pods are sorted, nos computes the aggregated requested resources by summing the request of each Pod, and it marks as over-quota all the pods for which used is greater than min . \ud83d\udea7 Soon it will be possible to customize the order criteria used for sorting the pods during this process through the nos-operator configuration. Over-quota fair sharing In order to prevent a single namespace from consuming all the over-quotas available in the cluster and starving the others, nos implements a fair-sharing mechanism that guarantees that each namespace subject to an ElasticQuota has right to a part of the available over-quotas proportional to its min field. The fair-sharing mechanism does not enforce any hard limit on the amount of over-quotas pods that a namespace can have, but instead it implements fair sharing by preemption. Specifically, a Pod-A subject to elastic-quota-A can preempt Pod-b subject to elastic-quota-B if the following conditions are met: Pod-B is in over-quota used field of Elastic-quota-A + Pod-A request <= guaranteed over-quotas A used over-quotas of Elastic-quota-B > guaranteed over-quotas B Where: guaranteed over-quotas A = percentage of guaranteed over-quotas A * tot. available over-quotas percentage of guaranteed over-quotas A = min A / sum(min_i) * 100 tot. available over-quotas = sum( max(0, min_i - used_i ) ) Example Let's assume we have a K8s cluster with the following Elastic Quota resources: Elastic Quota Min Max Elastic Quota A nos.nebuly.com/gpu-memory: 40 None Elastic Quota B nos.nebuly.com/gpu-memory: 10 None Elastic Quota C nos.nebuly.com/gpu-memory: 30 None The table below shows the quotas usage of the cluster at two different times: Time Elastic Quota A Elastic Quota B Elastic Quota C t1 Used: 40/40 GB Used: 40/10 GB Over-quota: 30 GB Used: 0 GB t2 Used: 50/40 GB Used 30/10 GB Over-quota: 20 GB Used: 0 GB The cluster has a total of 30 GB of memory of available over-quotas, which at time t1 are all being consumed by the pods in the namespace subject to Elastic Quota B. At time t2 , a new Pod is created in the namespace subject to Elastic Quota A. Even though all the quotas of the cluster are currently being used, the fair sharing mechanism grants to Elastic Quota A a certain amount of over-quotas that it can use, and in order to grant these quotas nos can preempt one or more over-quota pods from the namespace subject to Elastic Quota B. Specifically, the following are the amounts of over-quotas guaranteed to each of the namespaces subject to the Elastic Quotas defined in the table above: guaranteed over-quota A = 40 / (40 + 10 + 30) * (0 + 0 + (30 - 0)) = 15 guaranteed over-quota B = 10 / (40 + 10 + 30) * (0 + 0 + (30 - 0)) = 3 Assuming that all the pods in the cluster are requesting only 10 GB of GPU memory, an over-quota Pod from Elastic Quota B is preempted because the following conditions are true: \u2705 used quotas A + new Pod A <= min quota A + guaranteed over-quota A 40 + 10 <= 40 + 15 \u2705 used over-quotas B > guaranteed over-quotas 30 > 3 GPU memory limits Both ElasticQuota and CompositeElasticQuota resources support the custom resource nos.nebuly.com/gpu-memory . You can use this resource in the min and max fields of the elastic quotas specification to define the minimum amount of GPU memory (expressed in GB) guaranteed to a certain namespace and its maximum limit, respectively. This resource is particularly useful if you use Elastic Quotas together with automatic GPU partitioning , since it allows you to assign resources to different teams (e.g. namespaces) in terms of GPU memory instead of in number of GPUs, and the users can than consume request in the same terms by claiming GPU slices with a specific amount of memory, enabling an overall fine-grained control over the GPUs of the cluster. nos automatically computes the GPU memory requested by each Pod from the GPU resources requested by its containers and enforces the limits accordingly. The amount of memory GB corresponding to the generic resource nvidia.com/gpu is defined by the field global.nvidiaGpuResourceMemoryGB of the installation chart, which is 32 by default. For instance, using the default configuration, the value of the resource nos.nebuly.com/gpu-memory computed from the Pod specification below is 10+32=42 . apiVersion: apps/v1 kind: Pod metadata: name: nginx-deployment spec: schedulerName: nos-scheduler containers: - name: my-container image: my-image:0.0.1 resources: limits: nvidia.com/mig-1g.10gb: 1 nvidia.com/gpu: 1","title":"Key concepts"},{"location":"elastic-resource-quota/key-concepts/#key-concepts","text":"","title":"Key concepts"},{"location":"elastic-resource-quota/key-concepts/#over-quotas","text":"If a namespace subject to an ElasticQuota (or, equivalently, to a CompositeElasticQuota ) is using all the resources guaranteed by the min field of its quota, it can still host new pods by \"borrowing\" quotas from other namespaces which has available resources (e.g. from namespaces subject to other quotas where min resources are not being completely used). !!! info Pods that are scheduled \"borrowing\" unused quotas from other namespaces are called **over-quota pods**. Over-quota pods can be preempted at any time to free up resources if any of the namespaces lending the quotas claims back its resources. You can check whether a Pod is in over-quota by checking the value of the label nos.nebuly.com/capacity , which is automatically created and updated by the nos operator for every Pod created in a namespace subject to an ElasticQuota or to a CompositeElasticQuota. The two possible values for this label are in-quota and over-quota . You can use this label to easily find out at any time which are the over-quota pods subject to preemption risk: kubectl get pods --all-namespaces -l nos.nebuly.com/capacity=\"over-quota\"","title":"Over-quotas"},{"location":"elastic-resource-quota/key-concepts/#how-over-quota-pods-are-labelled","text":"All the pods created within a namespace subject to a quota are labelled as in-quota as long as the used resources of the quota do not exceed its min resources. When this happens and news pods are created in that namespace, they are labelled as over-quota when they reach the running phase. nos re-evaluates the over-quota status of each Pod of a namespace every time a new Pod in that namespace changes its phase to/from \"Running\". With the default configuration, nos sorts the pods by creation date and, if the creation timestamp is the same, by requested resources, placing first the pods with older creation timestamp and with fewer requested resources. After the pods are sorted, nos computes the aggregated requested resources by summing the request of each Pod, and it marks as over-quota all the pods for which used is greater than min . \ud83d\udea7 Soon it will be possible to customize the order criteria used for sorting the pods during this process through the nos-operator configuration.","title":"How over-quota pods are labelled"},{"location":"elastic-resource-quota/key-concepts/#over-quota-fair-sharing","text":"In order to prevent a single namespace from consuming all the over-quotas available in the cluster and starving the others, nos implements a fair-sharing mechanism that guarantees that each namespace subject to an ElasticQuota has right to a part of the available over-quotas proportional to its min field. The fair-sharing mechanism does not enforce any hard limit on the amount of over-quotas pods that a namespace can have, but instead it implements fair sharing by preemption. Specifically, a Pod-A subject to elastic-quota-A can preempt Pod-b subject to elastic-quota-B if the following conditions are met: Pod-B is in over-quota used field of Elastic-quota-A + Pod-A request <= guaranteed over-quotas A used over-quotas of Elastic-quota-B > guaranteed over-quotas B Where: guaranteed over-quotas A = percentage of guaranteed over-quotas A * tot. available over-quotas percentage of guaranteed over-quotas A = min A / sum(min_i) * 100 tot. available over-quotas = sum( max(0, min_i - used_i ) )","title":"Over-quota fair sharing"},{"location":"elastic-resource-quota/key-concepts/#example","text":"Let's assume we have a K8s cluster with the following Elastic Quota resources: Elastic Quota Min Max Elastic Quota A nos.nebuly.com/gpu-memory: 40 None Elastic Quota B nos.nebuly.com/gpu-memory: 10 None Elastic Quota C nos.nebuly.com/gpu-memory: 30 None The table below shows the quotas usage of the cluster at two different times: Time Elastic Quota A Elastic Quota B Elastic Quota C t1 Used: 40/40 GB Used: 40/10 GB Over-quota: 30 GB Used: 0 GB t2 Used: 50/40 GB Used 30/10 GB Over-quota: 20 GB Used: 0 GB The cluster has a total of 30 GB of memory of available over-quotas, which at time t1 are all being consumed by the pods in the namespace subject to Elastic Quota B. At time t2 , a new Pod is created in the namespace subject to Elastic Quota A. Even though all the quotas of the cluster are currently being used, the fair sharing mechanism grants to Elastic Quota A a certain amount of over-quotas that it can use, and in order to grant these quotas nos can preempt one or more over-quota pods from the namespace subject to Elastic Quota B. Specifically, the following are the amounts of over-quotas guaranteed to each of the namespaces subject to the Elastic Quotas defined in the table above: guaranteed over-quota A = 40 / (40 + 10 + 30) * (0 + 0 + (30 - 0)) = 15 guaranteed over-quota B = 10 / (40 + 10 + 30) * (0 + 0 + (30 - 0)) = 3 Assuming that all the pods in the cluster are requesting only 10 GB of GPU memory, an over-quota Pod from Elastic Quota B is preempted because the following conditions are true: \u2705 used quotas A + new Pod A <= min quota A + guaranteed over-quota A 40 + 10 <= 40 + 15 \u2705 used over-quotas B > guaranteed over-quotas 30 > 3","title":"Example"},{"location":"elastic-resource-quota/key-concepts/#gpu-memory-limits","text":"Both ElasticQuota and CompositeElasticQuota resources support the custom resource nos.nebuly.com/gpu-memory . You can use this resource in the min and max fields of the elastic quotas specification to define the minimum amount of GPU memory (expressed in GB) guaranteed to a certain namespace and its maximum limit, respectively. This resource is particularly useful if you use Elastic Quotas together with automatic GPU partitioning , since it allows you to assign resources to different teams (e.g. namespaces) in terms of GPU memory instead of in number of GPUs, and the users can than consume request in the same terms by claiming GPU slices with a specific amount of memory, enabling an overall fine-grained control over the GPUs of the cluster. nos automatically computes the GPU memory requested by each Pod from the GPU resources requested by its containers and enforces the limits accordingly. The amount of memory GB corresponding to the generic resource nvidia.com/gpu is defined by the field global.nvidiaGpuResourceMemoryGB of the installation chart, which is 32 by default. For instance, using the default configuration, the value of the resource nos.nebuly.com/gpu-memory computed from the Pod specification below is 10+32=42 . apiVersion: apps/v1 kind: Pod metadata: name: nginx-deployment spec: schedulerName: nos-scheduler containers: - name: my-container image: my-image:0.0.1 resources: limits: nvidia.com/mig-1g.10gb: 1 nvidia.com/gpu: 1","title":"GPU memory limits"},{"location":"elastic-resource-quota/overview/","text":"Overview nos extends the Kubernetes Resource Quotas by implementing the Capacity Scheduling KEP and adding more flexibility through two custom resources: ElasticQuotas and CompositeElasticQuotas . While standard Kubernetes resource quotas allow you only to define limits on the maximum overall resource allocation of each namespace, nos elastic quotas let you define two different limits: min : the minimum resources that are guaranteed to the namespace max : the upper bound of the resources that the namespace can consume In this way namespaces can borrow reserved resource quotas from other namespaces that are not using them, as long as they do not exceed their max limit (if any) and the namespaces lending the quotas do not need them. When a namespace claims back its reserved min resources, pods borrowing resources from other namespaces (e.g. over-quota pods) are preempted to make up space. Moreover, while the standard Kubernetes quota management computes the used quotas as the aggregation of the resources of the resource requests specified in the Pods spec, nos computes the used quotas by taking into account only running Pods in order to avoid lower resource utilization due to scheduled Pods that failed to start. Elastic Resource Quota management is based on the Capacity Scheduling scheduler plugin, which also implements the Capacity Scheduling KEP . nos extends the former implementation by adding the following features: over-quota pods preemption CompositeElasticQuota resources for defining limits on multiple namespaces custom resource nos.nebuly.com/gpu-memory fair sharing of over-quota resources optional max limits","title":"Overview"},{"location":"elastic-resource-quota/overview/#overview","text":"nos extends the Kubernetes Resource Quotas by implementing the Capacity Scheduling KEP and adding more flexibility through two custom resources: ElasticQuotas and CompositeElasticQuotas . While standard Kubernetes resource quotas allow you only to define limits on the maximum overall resource allocation of each namespace, nos elastic quotas let you define two different limits: min : the minimum resources that are guaranteed to the namespace max : the upper bound of the resources that the namespace can consume In this way namespaces can borrow reserved resource quotas from other namespaces that are not using them, as long as they do not exceed their max limit (if any) and the namespaces lending the quotas do not need them. When a namespace claims back its reserved min resources, pods borrowing resources from other namespaces (e.g. over-quota pods) are preempted to make up space. Moreover, while the standard Kubernetes quota management computes the used quotas as the aggregation of the resources of the resource requests specified in the Pods spec, nos computes the used quotas by taking into account only running Pods in order to avoid lower resource utilization due to scheduled Pods that failed to start. Elastic Resource Quota management is based on the Capacity Scheduling scheduler plugin, which also implements the Capacity Scheduling KEP . nos extends the former implementation by adding the following features: over-quota pods preemption CompositeElasticQuota resources for defining limits on multiple namespaces custom resource nos.nebuly.com/gpu-memory fair sharing of over-quota resources optional max limits","title":"Overview"},{"location":"elastic-resource-quota/troubleshooting/","text":"Troubleshooting You can check the logs of the scheduler by running the following command: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-scheduler -f You can check the logs of the operator by running the following command: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-operator -f","title":"Troubleshooting"},{"location":"elastic-resource-quota/troubleshooting/#troubleshooting","text":"You can check the logs of the scheduler by running the following command: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-scheduler -f You can check the logs of the operator by running the following command: kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-operator -f","title":"Troubleshooting"},{"location":"helm-charts/nos/","text":"nos The open-source platform for running AI workloads on k8s in an optimized way, both in terms of hardware utilization and workload performance. Maintainers Name Email Url Michele Zanotti m.zanotti@nebuly.com Diego Fiori d.fiori@nebuly.com Source Code https://github.com/nebuly-ai/nos Values Key Type Default Description allowDefaultNamespace bool false If true allows to deploy nos chart in the default namespace gpuPartitioner.affinity object {} Sets the affinity config of the GPU Partitioner Pod. gpuPartitioner.batchWindowIdleSeconds int 10 Idle seconds before the GPU partitioner processes the current batch if no new pending Pods are created, and the timeout has not been reached. Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.batchWindowTimeoutSeconds int 60 Timeout of the window used by the GPU partitioner for batching pending Pods. Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.devicePlugin.config.name string \"nos-device-plugin-configs\" Name of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the value \"devicePlugin.config.name\" of the Helm chart used for deploying the NVIDIA GPU Operator. gpuPartitioner.devicePlugin.config.namespace string \"nebuly-nvidia\" Namespace of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the namespace where the Nebuly NVIDIA Device Plugin has been deployed to. gpuPartitioner.devicePlugin.configUpdateDelaySeconds int 5 Duration of the delay between when the new partitioning config is computed and when it is sent to the NVIDIA device plugin. Since the config is provided to the plugin as a mounted ConfigMap, this delay is required to ensure that the updated ConfigMap is propagated to the mounted volume. gpuPartitioner.enabled bool true Enable or disable the nos gpu partitioner gpuPartitioner.fullnameOverride string \"\" gpuPartitioner.gpuAgent object - Configuration of the GPU Agent component of the GPU Partitioner. gpuPartitioner.gpuAgent.image.pullPolicy string \"IfNotPresent\" Sets the GPU Agent Docker image pull policy. gpuPartitioner.gpuAgent.image.repository string \"ghcr.io/nebuly-ai/nos-gpu-agent\" Sets the GPU Agent Docker image. gpuPartitioner.gpuAgent.image.tag string \"\" Overrides the GPU Agent image tag whose default is the chart appVersion. gpuPartitioner.gpuAgent.logLevel int 0 The level of log of the GPU Agent. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . gpuPartitioner.gpuAgent.reportConfigIntervalSeconds int 10 Interval at which the mig-agent will report to k8s status of the GPUs of the Node gpuPartitioner.gpuAgent.resources object {\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}} Sets the resource requests and limits of the GPU Agent container. gpuPartitioner.gpuAgent.runtimeClassName string nil The container runtime class name to use for the GPU Agent container. gpuPartitioner.gpuAgent.tolerations list [{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}] Sets the tolerations of the GPU Agent Pod. gpuPartitioner.image.pullPolicy string \"IfNotPresent\" Sets the GPU Partitioner Docker image pull policy. gpuPartitioner.image.repository string \"ghcr.io/nebuly-ai/nos-gpu-partitioner\" Sets the GPU Partitioner Docker image. gpuPartitioner.image.tag string \"\" Overrides the GPU Partitioner image tag whose default is the chart appVersion. gpuPartitioner.knownMigGeometries list - List that associates GPU models to the respective allowed MIG configurations gpuPartitioner.kubeRbacProxy object - Configuration of the Kube RBAC Proxy , which runs as sidecar of all the GPU Partitioner components Pods. gpuPartitioner.leaderElection.enabled bool true Enables/Disables the leader election of the GPU Partitioner controller manager. gpuPartitioner.logLevel int 0 The level of log of the GPU Partitioner. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . gpuPartitioner.migAgent object - Configuration of the MIG Agent component of the GPU Partitioner. gpuPartitioner.migAgent.image.pullPolicy string \"IfNotPresent\" Sets the MIG Agent Docker image pull policy. gpuPartitioner.migAgent.image.repository string \"ghcr.io/nebuly-ai/nos-mig-agent\" Sets the MIG Agent Docker image. gpuPartitioner.migAgent.image.tag string \"\" Overrides the MIG Agent image tag whose default is the chart appVersion. gpuPartitioner.migAgent.logLevel int 0 The level of log of the MIG Agent. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . gpuPartitioner.migAgent.reportConfigIntervalSeconds int 10 Interval at which the mig-agent will report to k8s the MIG partitioning status of the GPUs of the Node gpuPartitioner.migAgent.resources object {\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}} Sets the resource requests and limits of the MIG Agent container. gpuPartitioner.migAgent.tolerations list [{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}] Sets the tolerations of the MIG Agent Pod. gpuPartitioner.nameOverride string \"\" gpuPartitioner.nodeSelector object {} Sets the nodeSelector config of the GPU Partitioner Pod. gpuPartitioner.podAnnotations object {} Sets the annotations of the GPU Partitioner Pod. gpuPartitioner.podSecurityContext object {\"runAsNonRoot\":true,\"runAsUser\":1000} Sets the security context of the GPU partitioner Pod. gpuPartitioner.replicaCount int 1 Number of replicas of the gpu-manager Pod. gpuPartitioner.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}} Sets the resource limits and requests of the GPU partitioner container. gpuPartitioner.scheduler.config.name string \"nos-scheduler-config\" Name of the ConfigMap containing the k8s scheduler configuration file. If not specified or the ConfigMap does not exist, the GPU partitioner will use the default k8s scheduler profile. gpuPartitioner.tolerations list [] Sets the tolerations of the GPU Partitioner Pod. nvidiaGpuResourceMemoryGB int 32 Defines how many GB of memory each nvidia.com/gpu resource has. operator.affinity object {} Sets the affinity config of the operator Pod. operator.enabled bool true Enable or disable the nos operator operator.fullnameOverride string \"\" operator.image.pullPolicy string \"IfNotPresent\" Sets the operator Docker image pull policy. operator.image.repository string \"ghcr.io/nebuly-ai/nos-operator\" Sets the operator Docker repository operator.image.tag string \"\" Overrides the operator Docker image tag whose default is the chart appVersion. operator.kubeRbacProxy object - Configuration of the Kube RBAC Proxy , which runs as sidecar of the operator Pods. operator.leaderElection.enabled bool true Enables/Disables the leader election of the operator controller manager. operator.logLevel int 0 The level of log of the controller manager. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . operator.nameOverride string \"\" operator.nodeSelector object {} Sets the nodeSelector config of the operator Pod. operator.podAnnotations object {} Sets the annotations of the operator Pod. operator.podSecurityContext object {\"runAsNonRoot\":true} Sets the security context of the operator Pod. operator.replicaCount int 1 Number of replicas of the controller manager Pod. operator.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}} Sets the resource limits and requests of the operator controller manager container. operator.securityContext object {\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]}} Sets the security context of the operator container. operator.tolerations list [] Sets the tolerations of the operator Pod. scheduler.affinity object {} Sets the affinity config of the scheduler deployment. scheduler.config object {} Overrides the Kube Scheduler configuration scheduler.enabled bool true Enable or disable the nos scheduler scheduler.fullnameOverride string \"\" scheduler.image.pullPolicy string \"IfNotPresent\" Sets Docker image pull policy. scheduler.image.repository string \"ghcr.io/nebuly-ai/nos-scheduler\" Sets Docker image. scheduler.image.tag string \"\" Overrides the image tag whose default is the chart appVersion. scheduler.leaderElection.enabled bool true Enables/Disables the leader election when deployed with multiple replicas. scheduler.logLevel int 0 The level of log of the scheduler. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . scheduler.nameOverride string \"\" scheduler.nodeSelector object {} Sets the nodeSelector config of the scheduler deployment. scheduler.podAnnotations object {} Sets the annotations of the scheduler Pod. scheduler.podSecurityContext object {} Sets the security context of the scheduler Pod scheduler.replicaCount int 1 Number of replicas of the scheduler. scheduler.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}} Sets the resource limits and requests of the scheduler container. scheduler.securityContext object {\"privileged\":false} Sets the security context of the scheduler container scheduler.tolerations list [] Sets the tolerations of the scheduler deployment. shareTelemetry bool true If true, shares with Nebuly telemetry data collected only during the Chart installation","title":"nos"},{"location":"helm-charts/nos/#nos","text":"The open-source platform for running AI workloads on k8s in an optimized way, both in terms of hardware utilization and workload performance.","title":"nos"},{"location":"helm-charts/nos/#maintainers","text":"Name Email Url Michele Zanotti m.zanotti@nebuly.com Diego Fiori d.fiori@nebuly.com","title":"Maintainers"},{"location":"helm-charts/nos/#source-code","text":"https://github.com/nebuly-ai/nos","title":"Source Code"},{"location":"helm-charts/nos/#values","text":"Key Type Default Description allowDefaultNamespace bool false If true allows to deploy nos chart in the default namespace gpuPartitioner.affinity object {} Sets the affinity config of the GPU Partitioner Pod. gpuPartitioner.batchWindowIdleSeconds int 10 Idle seconds before the GPU partitioner processes the current batch if no new pending Pods are created, and the timeout has not been reached. Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.batchWindowTimeoutSeconds int 60 Timeout of the window used by the GPU partitioner for batching pending Pods. Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.devicePlugin.config.name string \"nos-device-plugin-configs\" Name of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the value \"devicePlugin.config.name\" of the Helm chart used for deploying the NVIDIA GPU Operator. gpuPartitioner.devicePlugin.config.namespace string \"nebuly-nvidia\" Namespace of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the namespace where the Nebuly NVIDIA Device Plugin has been deployed to. gpuPartitioner.devicePlugin.configUpdateDelaySeconds int 5 Duration of the delay between when the new partitioning config is computed and when it is sent to the NVIDIA device plugin. Since the config is provided to the plugin as a mounted ConfigMap, this delay is required to ensure that the updated ConfigMap is propagated to the mounted volume. gpuPartitioner.enabled bool true Enable or disable the nos gpu partitioner gpuPartitioner.fullnameOverride string \"\" gpuPartitioner.gpuAgent object - Configuration of the GPU Agent component of the GPU Partitioner. gpuPartitioner.gpuAgent.image.pullPolicy string \"IfNotPresent\" Sets the GPU Agent Docker image pull policy. gpuPartitioner.gpuAgent.image.repository string \"ghcr.io/nebuly-ai/nos-gpu-agent\" Sets the GPU Agent Docker image. gpuPartitioner.gpuAgent.image.tag string \"\" Overrides the GPU Agent image tag whose default is the chart appVersion. gpuPartitioner.gpuAgent.logLevel int 0 The level of log of the GPU Agent. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . gpuPartitioner.gpuAgent.reportConfigIntervalSeconds int 10 Interval at which the mig-agent will report to k8s status of the GPUs of the Node gpuPartitioner.gpuAgent.resources object {\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}} Sets the resource requests and limits of the GPU Agent container. gpuPartitioner.gpuAgent.runtimeClassName string nil The container runtime class name to use for the GPU Agent container. gpuPartitioner.gpuAgent.tolerations list [{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}] Sets the tolerations of the GPU Agent Pod. gpuPartitioner.image.pullPolicy string \"IfNotPresent\" Sets the GPU Partitioner Docker image pull policy. gpuPartitioner.image.repository string \"ghcr.io/nebuly-ai/nos-gpu-partitioner\" Sets the GPU Partitioner Docker image. gpuPartitioner.image.tag string \"\" Overrides the GPU Partitioner image tag whose default is the chart appVersion. gpuPartitioner.knownMigGeometries list - List that associates GPU models to the respective allowed MIG configurations gpuPartitioner.kubeRbacProxy object - Configuration of the Kube RBAC Proxy , which runs as sidecar of all the GPU Partitioner components Pods. gpuPartitioner.leaderElection.enabled bool true Enables/Disables the leader election of the GPU Partitioner controller manager. gpuPartitioner.logLevel int 0 The level of log of the GPU Partitioner. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . gpuPartitioner.migAgent object - Configuration of the MIG Agent component of the GPU Partitioner. gpuPartitioner.migAgent.image.pullPolicy string \"IfNotPresent\" Sets the MIG Agent Docker image pull policy. gpuPartitioner.migAgent.image.repository string \"ghcr.io/nebuly-ai/nos-mig-agent\" Sets the MIG Agent Docker image. gpuPartitioner.migAgent.image.tag string \"\" Overrides the MIG Agent image tag whose default is the chart appVersion. gpuPartitioner.migAgent.logLevel int 0 The level of log of the MIG Agent. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . gpuPartitioner.migAgent.reportConfigIntervalSeconds int 10 Interval at which the mig-agent will report to k8s the MIG partitioning status of the GPUs of the Node gpuPartitioner.migAgent.resources object {\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}} Sets the resource requests and limits of the MIG Agent container. gpuPartitioner.migAgent.tolerations list [{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}] Sets the tolerations of the MIG Agent Pod. gpuPartitioner.nameOverride string \"\" gpuPartitioner.nodeSelector object {} Sets the nodeSelector config of the GPU Partitioner Pod. gpuPartitioner.podAnnotations object {} Sets the annotations of the GPU Partitioner Pod. gpuPartitioner.podSecurityContext object {\"runAsNonRoot\":true,\"runAsUser\":1000} Sets the security context of the GPU partitioner Pod. gpuPartitioner.replicaCount int 1 Number of replicas of the gpu-manager Pod. gpuPartitioner.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}} Sets the resource limits and requests of the GPU partitioner container. gpuPartitioner.scheduler.config.name string \"nos-scheduler-config\" Name of the ConfigMap containing the k8s scheduler configuration file. If not specified or the ConfigMap does not exist, the GPU partitioner will use the default k8s scheduler profile. gpuPartitioner.tolerations list [] Sets the tolerations of the GPU Partitioner Pod. nvidiaGpuResourceMemoryGB int 32 Defines how many GB of memory each nvidia.com/gpu resource has. operator.affinity object {} Sets the affinity config of the operator Pod. operator.enabled bool true Enable or disable the nos operator operator.fullnameOverride string \"\" operator.image.pullPolicy string \"IfNotPresent\" Sets the operator Docker image pull policy. operator.image.repository string \"ghcr.io/nebuly-ai/nos-operator\" Sets the operator Docker repository operator.image.tag string \"\" Overrides the operator Docker image tag whose default is the chart appVersion. operator.kubeRbacProxy object - Configuration of the Kube RBAC Proxy , which runs as sidecar of the operator Pods. operator.leaderElection.enabled bool true Enables/Disables the leader election of the operator controller manager. operator.logLevel int 0 The level of log of the controller manager. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . operator.nameOverride string \"\" operator.nodeSelector object {} Sets the nodeSelector config of the operator Pod. operator.podAnnotations object {} Sets the annotations of the operator Pod. operator.podSecurityContext object {\"runAsNonRoot\":true} Sets the security context of the operator Pod. operator.replicaCount int 1 Number of replicas of the controller manager Pod. operator.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}} Sets the resource limits and requests of the operator controller manager container. operator.securityContext object {\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]}} Sets the security context of the operator container. operator.tolerations list [] Sets the tolerations of the operator Pod. scheduler.affinity object {} Sets the affinity config of the scheduler deployment. scheduler.config object {} Overrides the Kube Scheduler configuration scheduler.enabled bool true Enable or disable the nos scheduler scheduler.fullnameOverride string \"\" scheduler.image.pullPolicy string \"IfNotPresent\" Sets Docker image pull policy. scheduler.image.repository string \"ghcr.io/nebuly-ai/nos-scheduler\" Sets Docker image. scheduler.image.tag string \"\" Overrides the image tag whose default is the chart appVersion. scheduler.leaderElection.enabled bool true Enables/Disables the leader election when deployed with multiple replicas. scheduler.logLevel int 0 The level of log of the scheduler. Zero corresponds to info , while values greater or equal than 1 corresponds to higher debug levels. Must be >= 0 . scheduler.nameOverride string \"\" scheduler.nodeSelector object {} Sets the nodeSelector config of the scheduler deployment. scheduler.podAnnotations object {} Sets the annotations of the scheduler Pod. scheduler.podSecurityContext object {} Sets the security context of the scheduler Pod scheduler.replicaCount int 1 Number of replicas of the scheduler. scheduler.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}} Sets the resource limits and requests of the scheduler container. scheduler.securityContext object {\"privileged\":false} Sets the security context of the scheduler container scheduler.tolerations list [] Sets the tolerations of the scheduler deployment. shareTelemetry bool true If true, shares with Nebuly telemetry data collected only during the Chart installation","title":"Values"}]}